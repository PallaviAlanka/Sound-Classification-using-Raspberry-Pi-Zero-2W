{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyNJdWLjEmS0x2Xgsgth4RMH"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oQXKcov34D8z","executionInfo":{"status":"ok","timestamp":1746547611463,"user_tz":-330,"elapsed":21988,"user":{"displayName":"Nataraj Gowd Velagana","userId":"01084766380569095704"}},"outputId":"7fe76b92-cc36-4fcc-9966-9d5b60a08c7c"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["import os\n","import numpy as np\n","import librosa\n","import tensorflow as tf\n","from sklearn.model_selection import train_test_split\n","from tensorflow.keras.utils import to_categorical"],"metadata":{"id":"_lU89ksG4LIV","executionInfo":{"status":"ok","timestamp":1746550448455,"user_tz":-330,"elapsed":4,"user":{"displayName":"Nataraj Gowd Velagana","userId":"01084766380569095704"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["DATASET_PATH = \"/content/drive/MyDrive/IOT/SoundClasification/Dataset\"  # Change to your dataset path\n","SAMPLE_RATE = 16000\n","DURATION = 9  # second\n","N_MFCC = 13\n","TARGET_FRAMES = 260\n","NUM_CLASSES = 8  # Change if different\n","BATCH_SIZE = 16\n","EPOCHS = 20"],"metadata":{"id":"eCieenqxCC53"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"-FDXkUC_CG7h"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# =============================\n","# CONFIG\n","# =============================\n","DATASET_PATH = \"/content/drive/MyDrive/IOT/SoundClasification/AgumentedData\"  # Change to your dataset path\n","SAMPLE_RATE = 16000\n","DURATION = 9  # second\n","N_MFCC = 13\n","TARGET_FRAMES = 260\n","NUM_CLASSES = 7  # Change if different\n","BATCH_SIZE = 16\n","EPOCHS = 20\n","\n","# =============================\n","# Load and Preprocess Dataset\n","# =============================\n","def extract_mfcc(file_path):\n","    audio, sr = librosa.load(file_path, sr=SAMPLE_RATE, duration=DURATION)\n","    if len(audio) < SAMPLE_RATE * DURATION:\n","        padding = SAMPLE_RATE * DURATION - len(audio)\n","        audio = np.pad(audio, (0, padding))\n","    mfcc = librosa.feature.mfcc(y=audio, sr=sr, n_mfcc=N_MFCC)\n","    if mfcc.shape[1] < TARGET_FRAMES:\n","        pad_width = TARGET_FRAMES - mfcc.shape[1]\n","        mfcc = np.pad(mfcc, ((0, 0), (0, pad_width)), mode='constant')\n","    else:\n","        mfcc = mfcc[:, :TARGET_FRAMES]\n","    return mfcc.reshape(N_MFCC, TARGET_FRAMES, 1)\n","\n","from tqdm import tqdm  # Add this import at the top\n","\n","def load_dataset():\n","    X, y = [], []\n","    class_names = sorted(os.listdir(DATASET_PATH))\n","    label_map = {name: idx for idx, name in enumerate(class_names)}\n","\n","    total_files = sum([len(files) for _, _, files in os.walk(DATASET_PATH)])\n","\n","    with tqdm(total=total_files, desc=\"Loading dataset\", unit=\"file\") as pbar:\n","        for class_name in class_names:\n","            class_dir = os.path.join(DATASET_PATH, class_name)\n","            for file in os.listdir(class_dir):\n","                if file.endswith(\".wav\"):\n","                    file_path = os.path.join(class_dir, file)\n","                    mfcc = extract_mfcc(file_path)\n","                    X.append(mfcc)\n","                    y.append(label_map[class_name])\n","                    pbar.update(1)\n","\n","    X = np.array(X, dtype=np.float32)\n","    y = to_categorical(np.array(y), num_classes=NUM_CLASSES)\n","    return X, y, label_map\n","\n","\n","# =============================\n","# Model Definition\n","# =============================\n","def build_cnn_patch_embedding(input_shape=(13, 260, 1), embed_dim=256):\n","    model = tf.keras.Sequential([\n","        tf.keras.layers.Input(shape=input_shape),\n","        tf.keras.layers.Conv2D(32, (3, 3), activation='relu', padding='same'),\n","        tf.keras.layers.MaxPooling2D((2, 2)),\n","        tf.keras.layers.Conv2D(64, (3, 3), activation='relu', padding='same'),\n","        tf.keras.layers.MaxPooling2D((2, 2)),\n","        tf.keras.layers.Flatten(),\n","        tf.keras.layers.Dense(128, activation='relu'),\n","        tf.keras.layers.Dense(64, activation='relu'),\n","        tf.keras.layers.Dense(NUM_CLASSES, activation='softmax')\n","    ])\n","    return model\n","\n","# =============================\n","# Train the Model\n","# =============================\n","X, y, label_map = load_dataset()\n","X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, stratify=y)\n","\n","model = build_cnn_patch_embedding()\n","model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n","\n","# =============================\n","# Train the Model & Capture History\n","# =============================\n","history = model.fit(\n","    X_train, y_train,\n","    epochs=EPOCHS,\n","    batch_size=BATCH_SIZE,\n","    validation_data=(X_val, y_val)\n",")\n","\n","# =============================\n","# Print Final Accuracies\n","# =============================\n","final_train_acc = history.history['accuracy'][-1]\n","final_val_acc = history.history['val_accuracy'][-1]\n","\n","print(f\"Final Training Accuracy: {final_train_acc * 100:.2f}%\")\n","print(f\"Final Validation Accuracy: {final_val_acc * 100:.2f}%\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"j76tJ1cP4ZIJ","executionInfo":{"status":"ok","timestamp":1744376210956,"user_tz":-330,"elapsed":254353,"user":{"displayName":"Nataraj Gowd Velagana","userId":"01084766380569095704"}},"outputId":"6f45af94-173e-40eb-ccb6-792aaffb950d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Loading dataset:  71%|███████   | 399/563 [03:58<01:38,  1.67file/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/20\n","\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 188ms/step - accuracy: 0.1617 - loss: 57.8178 - val_accuracy: 0.2750 - val_loss: 1.7526\n","Epoch 2/20\n","\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5147 - loss: 1.3391 - val_accuracy: 0.7750 - val_loss: 0.6651\n","Epoch 3/20\n","\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9174 - loss: 0.3373 - val_accuracy: 0.7250 - val_loss: 0.6592\n","Epoch 4/20\n","\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9513 - loss: 0.1400 - val_accuracy: 0.8375 - val_loss: 0.5076\n","Epoch 5/20\n","\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9778 - loss: 0.0521 - val_accuracy: 0.8750 - val_loss: 0.3634\n","Epoch 6/20\n","\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9876 - loss: 0.0230 - val_accuracy: 0.8000 - val_loss: 0.5628\n","Epoch 7/20\n","\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9756 - loss: 0.0682 - val_accuracy: 0.9000 - val_loss: 0.3167\n","Epoch 8/20\n","\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9930 - loss: 0.0297 - val_accuracy: 0.8375 - val_loss: 0.7468\n","Epoch 9/20\n","\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9932 - loss: 0.0266 - val_accuracy: 0.9000 - val_loss: 0.3834\n","Epoch 10/20\n","\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9891 - loss: 0.0329 - val_accuracy: 0.8750 - val_loss: 0.3302\n","Epoch 11/20\n","\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 0.0065 - val_accuracy: 0.8500 - val_loss: 0.3616\n","Epoch 12/20\n","\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 0.0041 - val_accuracy: 0.8750 - val_loss: 0.3608\n","Epoch 13/20\n","\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 1.0000 - loss: 8.6366e-04 - val_accuracy: 0.8750 - val_loss: 0.3769\n","Epoch 14/20\n","\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 5.9176e-04 - val_accuracy: 0.8625 - val_loss: 0.3865\n","Epoch 15/20\n","\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 3.9467e-04 - val_accuracy: 0.8625 - val_loss: 0.3880\n","Epoch 16/20\n","\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 4.4174e-04 - val_accuracy: 0.8750 - val_loss: 0.3911\n","Epoch 17/20\n","\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 4.2472e-04 - val_accuracy: 0.8750 - val_loss: 0.3918\n","Epoch 18/20\n","\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 2.7459e-04 - val_accuracy: 0.8750 - val_loss: 0.3900\n","Epoch 19/20\n","\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 2.2828e-04 - val_accuracy: 0.8750 - val_loss: 0.3885\n","Epoch 20/20\n","\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 1.0000 - loss: 2.2036e-04 - val_accuracy: 0.9000 - val_loss: 0.3900\n","Final Training Accuracy: 100.00%\n","Final Validation Accuracy: 90.00%\n"]}]},{"cell_type":"code","source":["# model.save(\"/content/drive/MyDrive/IOT/SoundClasification/Model_keras/model_libarosa.h5\")\n","\n","# =============================\n","# Convert to TFLite\n","# =============================\n","def convert_to_tflite(model_path, output_path):\n","    model = tf.keras.models.load_model(model_path)\n","    converter = tf.lite.TFLiteConverter.from_keras_model(model)\n","    tflite_model = converter.convert()\n","    with open(output_path, \"wb\") as f:\n","        f.write(tflite_model)\n","\n","convert_to_tflite(\"/content/drive/MyDrive/IOT/SoundClasification/Model_keras/model_libarosa.h5\", \"/content/drive/MyDrive/IOT/SoundClasification/Model_keras/model_libarosa.tflite\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7U2U7Nv-4VIg","executionInfo":{"status":"ok","timestamp":1744376454151,"user_tz":-330,"elapsed":1627,"user":{"displayName":"Nataraj Gowd Velagana","userId":"01084766380569095704"}},"outputId":"1549ffc5-fa36-4df4-f03b-3bd5d0cbf9b5"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"]},{"output_type":"stream","name":"stdout","text":["Saved artifact at '/tmp/tmpjxrq61w6'. The following endpoints are available:\n","\n","* Endpoint 'serve'\n","  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(None, 13, 260, 1), dtype=tf.float32, name='input_layer')\n","Output Type:\n","  TensorSpec(shape=(None, 8), dtype=tf.float32, name=None)\n","Captures:\n","  132676635141392: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  132676635132368: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  132676635135440: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  132676635135056: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  132676635141584: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  132676635135824: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  132676635137552: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  132676635143312: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  132676635134864: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  132676635145424: TensorSpec(shape=(), dtype=tf.resource, name=None)\n"]}]},{"cell_type":"code","source":["# =============================\n","# CONFIG\n","# =============================\n","DATASET_PATH = \"/content/drive/MyDrive/IOT/SoundClasification/AgumentedData\"  # Change to your dataset path\n","SAMPLE_RATE = 16000\n","DURATION = 9  # second\n","N_MFCC = 13\n","TARGET_FRAMES = 260\n","NUM_CLASSES = 6  # Changed to 6 for your 6-class dataset\n","BATCH_SIZE = 16\n","EPOCHS = 20\n","\n","# =============================\n","# Load and Preprocess Dataset\n","# =============================\n","def extract_mfcc(file_path):\n","    audio, sr = librosa.load(file_path, sr=SAMPLE_RATE, duration=DURATION)\n","    if len(audio) < SAMPLE_RATE * DURATION:\n","        padding = SAMPLE_RATE * DURATION - len(audio)\n","        audio = np.pad(audio, (0, padding))\n","    mfcc = librosa.feature.mfcc(y=audio, sr=sr, n_mfcc=N_MFCC)\n","    if mfcc.shape[1] < TARGET_FRAMES:\n","        pad_width = TARGET_FRAMES - mfcc.shape[1]\n","        mfcc = np.pad(mfcc, ((0, 0), (0, pad_width)), mode='constant')\n","    else:\n","        mfcc = mfcc[:, :TARGET_FRAMES]\n","    return mfcc.reshape(N_MFCC, TARGET_FRAMES, 1)\n","\n","from tqdm import tqdm  # Add this import at the top\n","\n","def load_dataset():\n","    X, y = [], []\n","    class_names = sorted(os.listdir(DATASET_PATH))\n","    label_map = {name: idx for idx, name in enumerate(class_names)}\n","\n","    # If you want to explicitly add a \"no class\" folder (even if empty), it could be added manually\n","    if 'no_class' not in class_names:\n","        os.makedirs(os.path.join(DATASET_PATH, 'no_class'))  # Create a dummy 'no_class' folder\n","\n","    total_files = sum([len(files) for _, _, files in os.walk(DATASET_PATH)])\n","\n","    with tqdm(total=total_files, desc=\"Loading dataset\", unit=\"file\") as pbar:\n","        for class_name in class_names:\n","            class_dir = os.path.join(DATASET_PATH, class_name)\n","            for file in os.listdir(class_dir):\n","                if file.endswith(\".wav\"):\n","                    file_path = os.path.join(class_dir, file)\n","                    mfcc = extract_mfcc(file_path)\n","                    X.append(mfcc)\n","                    y.append(label_map.get(class_name, NUM_CLASSES-1))  # Default to 'no class' label\n","                    pbar.update(1)\n","\n","    X = np.array(X, dtype=np.float32)\n","    y = to_categorical(np.array(y), num_classes=NUM_CLASSES)\n","    return X, y, label_map\n","\n","# =============================\n","# Model Definition\n","# =============================\n","def build_cnn_patch_embedding(input_shape=(13, 260, 1), embed_dim=256):\n","    model = tf.keras.Sequential([\n","        tf.keras.layers.Input(shape=input_shape),\n","        tf.keras.layers.Conv2D(32, (3, 3), activation='relu', padding='same'),\n","        tf.keras.layers.MaxPooling2D((2, 2)),\n","        tf.keras.layers.Conv2D(64, (3, 3), activation='relu', padding='same'),\n","        tf.keras.layers.MaxPooling2D((2, 2)),\n","        tf.keras.layers.Flatten(),\n","        tf.keras.layers.Dense(128, activation='relu'),\n","        tf.keras.layers.Dense(64, activation='relu'),\n","        tf.keras.layers.Dense(NUM_CLASSES, activation='softmax')\n","    ])\n","    return model\n","\n","# =============================\n","# Train the Model\n","# =============================\n","X, y, label_map = load_dataset()\n","X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, stratify=y)\n","\n","model = build_cnn_patch_embedding()\n","model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n","\n","# =============================\n","# Train the Model & Capture History\n","# =============================\n","history = model.fit(\n","    X_train, y_train,\n","    epochs=EPOCHS,\n","    batch_size=BATCH_SIZE,\n","    validation_data=(X_val, y_val)\n",")\n","\n","# =============================\n","# Print Final Accuracies\n","# =============================\n","final_train_acc = history.history['accuracy'][-1]\n","final_val_acc = history.history['val_accuracy'][-1]\n","\n","print(f\"Final Training Accuracy: {final_train_acc * 100:.2f}%\")\n","print(f\"Final Validation Accuracy: {final_val_acc * 100:.2f}%\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TaPBNjHeWG-E","executionInfo":{"status":"ok","timestamp":1746550522592,"user_tz":-330,"elapsed":68144,"user":{"displayName":"Nataraj Gowd Velagana","userId":"01084766380569095704"}},"outputId":"29b10b3a-6ccc-4816-87f5-652f1b0c5e58"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stderr","text":["Loading dataset: 100%|██████████| 1196/1196 [00:44<00:00, 26.69file/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/20\n","\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 62ms/step - accuracy: 0.4368 - loss: 20.8393 - val_accuracy: 0.8417 - val_loss: 0.4312\n","Epoch 2/20\n","\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - accuracy: 0.9162 - loss: 0.2632 - val_accuracy: 0.9458 - val_loss: 0.1495\n","Epoch 3/20\n","\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9500 - loss: 0.1466 - val_accuracy: 0.9083 - val_loss: 0.2046\n","Epoch 4/20\n","\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9577 - loss: 0.1278 - val_accuracy: 0.9417 - val_loss: 0.1438\n","Epoch 5/20\n","\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9784 - loss: 0.0556 - val_accuracy: 0.9917 - val_loss: 0.0433\n","Epoch 6/20\n","\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9934 - loss: 0.0192 - val_accuracy: 0.9917 - val_loss: 0.0308\n","Epoch 7/20\n","\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9931 - loss: 0.0245 - val_accuracy: 0.9708 - val_loss: 0.0566\n","Epoch 8/20\n","\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 0.0048 - val_accuracy: 0.9917 - val_loss: 0.0427\n","Epoch 9/20\n","\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 0.0018 - val_accuracy: 0.9917 - val_loss: 0.0378\n","Epoch 10/20\n","\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 0.0011 - val_accuracy: 0.9917 - val_loss: 0.0534\n","Epoch 11/20\n","\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9669 - loss: 0.1037 - val_accuracy: 0.9708 - val_loss: 0.1176\n","Epoch 12/20\n","\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9558 - loss: 0.1414 - val_accuracy: 0.9375 - val_loss: 0.2136\n","Epoch 13/20\n","\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9600 - loss: 0.1485 - val_accuracy: 0.9500 - val_loss: 0.1331\n","Epoch 14/20\n","\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9775 - loss: 0.0843 - val_accuracy: 0.9917 - val_loss: 0.0164\n","Epoch 15/20\n","\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9855 - loss: 0.0743 - val_accuracy: 0.9750 - val_loss: 0.0448\n","Epoch 16/20\n","\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9928 - loss: 0.0124 - val_accuracy: 0.9917 - val_loss: 0.0224\n","Epoch 17/20\n","\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.9970 - loss: 0.0101 - val_accuracy: 0.9833 - val_loss: 0.0307\n","Epoch 18/20\n","\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9922 - loss: 0.0257 - val_accuracy: 0.9792 - val_loss: 0.0528\n","Epoch 19/20\n","\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.9975 - loss: 0.0163 - val_accuracy: 0.9750 - val_loss: 0.0613\n","Epoch 20/20\n","\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.9985 - loss: 0.0027 - val_accuracy: 0.9917 - val_loss: 0.0220\n","Final Training Accuracy: 99.90%\n","Final Validation Accuracy: 99.17%\n"]}]},{"cell_type":"code","source":["model.save(\"/content/drive/MyDrive/IOT/SoundClasification/Model_keras/model_libarosa_1.h5\")\n","\n","# =============================\n","# Convert to TFLite\n","# =============================\n","def convert_to_tflite(model_path, output_path):\n","    model = tf.keras.models.load_model(model_path)\n","    converter = tf.lite.TFLiteConverter.from_keras_model(model)\n","    tflite_model = converter.convert()\n","    with open(output_path, \"wb\") as f:\n","        f.write(tflite_model)\n","\n","convert_to_tflite(\"/content/drive/MyDrive/IOT/SoundClasification/Model_keras/model_libarosa_1.h5\", \"/content/drive/MyDrive/IOT/SoundClasification/Model_keras/model_libarosa_1.tflite\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qwBo7iibhPd1","executionInfo":{"status":"ok","timestamp":1746550627401,"user_tz":-330,"elapsed":1418,"user":{"displayName":"Nataraj Gowd Velagana","userId":"01084766380569095704"}},"outputId":"5e812d9e-28e2-4b64-c0e0-b25e2b842611"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stderr","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n","WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"]},{"output_type":"stream","name":"stdout","text":["Saved artifact at '/tmp/tmpk4q0jw31'. The following endpoints are available:\n","\n","* Endpoint 'serve'\n","  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(None, 13, 260, 1), dtype=tf.float32, name='input_layer_1')\n","Output Type:\n","  TensorSpec(shape=(None, 6), dtype=tf.float32, name=None)\n","Captures:\n","  135046512961168: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  135046512965584: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  135046512956560: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  135046512960208: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  135046516218320: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  135046516211792: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  135046516213328: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  135046516212944: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  135046516214672: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  135046516211984: TensorSpec(shape=(), dtype=tf.resource, name=None)\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"ABcLkEmph5V9"},"execution_count":null,"outputs":[]}]}